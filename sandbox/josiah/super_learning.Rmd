---
title: "Super Learning"
author: "Josiah Davis"
date: "4/12/2017"
output:
  pdf_document: default
  html_document: default
---

## Set up working environment
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(SuperLearner)
rm(list = ls()); gc()
set.seed(5000)
```

## Define convenience functions
```{r, message=FALSE}
get_data <- function(dir, file_in){
  # Get the HR data in data-frame format
  # 
  # Args:
  #   dir: working directory as character vector
  #   file_in: name of the file as character vector
  # 
  # Returns:
  #   dataframe object with no transformations
  
  df <- read.csv(paste0(dir, file_in), stringsAsFactors = FALSE)
  return(df)
}

bucket_covariate <- function(x, k){
  # Approximate a single quantitative variable based off of the quantiles
  # 
  # Args:
  #   x: The quantitative vector to be approximated
  #   k: The number of quantiles to use in the approximation
  # 
  # Returns:
  #   Numeric vector with numbers corresponding to quantiles (e.g., 1 = 1st quantile)
  
  x_split <- cut(x, breaks = quantile(x, probs = seq(0, 1, 1/k)), include.lowest = TRUE)
  if(any(is.na(x_split))) stop('There are NA values in')
  return(as.numeric(factor(x_split, labels = 1:k)))
}

pre_process_data <- function(df){
  # Handle all custodial details relating to the formatting of the data
  # This incluseds appriximating covariates, and removing strata that
  # violate the positivity assumption.
  # 
  # Args:
  #   df: observed data as a data-frame
  # 
  # Returns:
  #   Dataframe with variables ready for super learning
  
  # Remove people with medium salary level
  df <- df %>% filter(salary != 'medium')
  
  # Approximate all five quantitative variables as dichotomous variables
  df <- df %>% 
    mutate(
      number_project = bucket_covariate(number_project, 2)
      , average_montly_hours = bucket_covariate(average_montly_hours, 5)
      , time_spend_company = bucket_covariate(time_spend_company, 2)
    )
  
  # Check for violations of the positivity assumption
  df <- df %>% 
  group_by(
    number_project
    , average_montly_hours
    , time_spend_company
    , Work_accident
    , promotion_last_5years
    , sales
  ) %>% 
  mutate(
    both_treatments = (sum(salary == 'high') > 0) & (sum(salary == 'low') > 0)
    , salary = as.numeric(ifelse(salary == 'high', 1, 0))
  ) %>% ungroup()
  
  # Count up and print number of removed observations
  n_obs <- df %>% nrow()
  n_dropped_obs <- df %>% filter(!both_treatments) %>% nrow()
  cat('Number of positivity violations ', n_dropped_obs
      , ' (', round(100*n_dropped_obs/n_obs, 2), '%)', sep = '')
  
  # Drop observations from sample with positivity assumption violations
  df <- df %>% filter(both_treatments)
  
  # Generate dummy variables
  mm <- data.frame(model.matrix( ~ sales - 1, data = df))
  # Treat technical department as reference level
  mm <- mm[,!(colnames(mm) %in% 'salestechnical')]
  df <- cbind(df, mm)
  
  # Drop columns no longer needed
  df <- df %>% dplyr::select(-both_treatments, -sales)
  
  # Create attribute to store columns
  attr(df, 'x_cols') <- df %>% 
    dplyr::select(-left, -satisfaction_level, -last_evaluation) %>% 
    colnames()
  
  return(df)
}

get_super_learner <- function(df, learning_library){
  # Train SuperLearner and create counterfactual outcomes
  # 
  # Args:
  #   df: observed data as a data-frame
  #   learning_library: character vector of libraries used for ensembling
  # 
  # Returns:
  #   model object with predictions, and other results (e.g., cvRisk)

  
  # Set treatment to 0, 1 for generating the counterfactual outcomes
  X <-  df[,attr(df, 'x_cols')]
  X_0 <- X %>% mutate(salary = 0)
  X_1 <- X %>% mutate(salary = 1)
  
  # Run Super Learner
  model <- SuperLearner(Y = df$left
                        , X = df[,attr(df, 'x_cols')]
                        , newX = rbind(X, X_0, X_1)     # Note: this data is not used for training
                        , SL.library = learning_library
                        , cvControl = list(V = 5)
                        , family = 'binomial'
                        , verbose = FALSE)
  
  # Show some output
  print(model)
  run_time <- model$times$everything['elapsed'] %>% unname() / 60
  cat('Model training and predicting took', round(run_time, 2), 'minutes')
  
  return(model)
}

get_counterfact_outcomes <- function(df, model){
  # Retreive Y_a under each potential outcome from the super learner object
  # 
  # Args:
  #   df: observed data as a data-frame
  #   model: observed data as a data-frame
  # 
  # Returns:
  #   model object with predictions, and other results (e.g., cvRisk)

  
  df$Q_bar_AW <- model$SL.predict[1:nrow(df)]
  df$Q_bar_0W <- model$SL.predict[(nrow(df)+1):(2*nrow(df))]
  df$Q_bar_1W <- model$SL.predict[(2*nrow(df)+1):(3*nrow(df))]
  
  return(df)
}

save_output <- function(df, model, dir, out_name, save_model = TRUE){
  # Write counterfactual outcomes, model summary results to local disk
  # 
  # Args:
  #   df: observed data as a data-frame
  #   model: observed data as a data-frame
  #   dir: working directory as character vector
  #   out_name: name of output file with counterfactual outcomes as character vector
  #   save_model: Whether or not to save the full model as binary value
  # 
  # Returns:
  #   Nothing to the R environment
   
  # 1. Write dataframe containing counterfactual outcomes to csv
  df %>% write.csv(paste0(dir, out_name), row.names = FALSE)
  
  # 2. Save some modeling results
  model_summary <- list(
    risk = model$cvRisk
    , coefficients = model$coef
    , times = model$times
    )
  saveRDS(model_summary, paste0(dir, 'SL_summary'))

  # 3. Save full (large) super learning object
  if(save_model) saveRDS(model, paste0(dir, 'SL_full.rds'))
  
}

```

## Define Functions for Super Learning

```{r}
# Get knn with different sizes
create_SL_knn <- function(k = c(20, 30)) {
  for(mm in seq(length(k))){
    eval(parse(text = paste('SL.knn.', k[mm], '<- function(..., k = ', k[mm],
      ') SL.knn(..., k = k)', sep = '')), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create_SL_knn(c(10, 15, 20, 25))

# Get Neural networks of different sizes
create_SL_nnet <- function(size = c(2, 3)) {
  for(mm in seq(length(size))){
    eval(parse(text = paste('SL.nnet.', size[mm], '<- function(..., size = ', size[mm],
      ') SL.nnet(..., size = size)', sep = '')), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create_SL_nnet(c(2, 3, 4, 5, 6))

# Get both the ridge and lasso regressions
SL.glmnet.0 <- function(..., alpha = 0) SL.glmnet(..., alpha = 0) # Ridge
SL.glmnet.1 <- function(..., alpha = 1) SL.glmnet(..., alpha = 1) # Lasso

```

## Define constants
```{r}
# Set constants
SL_LIBRARY <- c(
  
  # Linear methods
  'SL.glm'
  , 'SL.glmnet.0' # Ridge
  , 'SL.glmnet.1' # Lasso
  
  # Additive models, Trees and other methods
  , 'SL.gam'
  , 'SL.xgboost'
  , 'SL.randomForest'
  , 'SL.rpartPrune'
  , 'SL.polymars'
  
  # Neural Network Methods
  , 'SL.nnet.2'
  , 'SL.nnet.3'
  , 'SL.nnet.4'
  , 'SL.nnet.5'
  , 'SL.nnet.6'

  # Prototype Methods
  , 'SL.knn.10'
  , 'SL.knn.15'
  , 'SL.knn.20'
  , 'SL.knn.25'
    
  # Other
  , 'SL.mean'
  )

DIR <- '/Users/josiahdavis/Documents/Berkeley/PH252D/data/' # <= UPDATE AS NEEDED
FILE_IN_NAME <- 'HR_comma_sep_2.csv'
FILE_OUT_NAME <- 'SL_output_2.csv'
```

## Estimate $Q_0(A,W)$
```{r, message=FALSE}

# Preprocess data including bucketing
obs_data <- get_data(DIR, FILE_IN_NAME)
obs_data <- pre_process_data(obs_data)

# Estimate Q_0(A,W)
q_hat_sl <- get_super_learner(obs_data, SL_LIBRARY)

print.SuperLearner

sl_summary <- data.frame(algorithm = names(q_hat_sl$cvRisk)
           , cvrisk = q_hat_sl$cvRisk
           , coefficient = q_hat_sl$coef)

# Estimate the risk for the super learner
cv_q_hat_sl <- CV.SuperLearner(Y=obs_data$left
                               , X=obs_data[,attr(obs_data, 'x_cols')]
                               , SL.library=SL_LIBRARY
                               , family='binomial'
                               , innerCvControl=list(V=10)
                               , cvControl=list(V=10))

# Calculate counterfactual outcomes
results <- get_counterfact_outcomes(obs_data, q_hat_sl)
```


## Estimate the treatment mechanisim: $g_0(A|W)$
```{r, eval=FALSE}
# Estimate propensity of treatment g_0(A|W)
g_hat_sl<- SuperLearner(Y=obs_data$salary, X=obs_data[,attr(obs_data, 'x_cols')] %>% select(-salary)
                      , SL.library=SL_LIBRARY, family="binomial")

g_hat1_W <- g_hat_sl$SL.predict
g_hat0_W <- 1 - g_hat1_W
g_hat_AW <- rep(NA, nrow(obs_data))
g_hat_AW[obs_data$salary==1] <- g_hat1_W[obs_data$salary==1]
g_hat_AW[obs_data$salary==0] <- g_hat0_W[obs_data$salary==0]
results$g_hat_AW <- g_hat_AW

head(results)
```


## Save results
```{r, eval=FALSE}

cv_q_hat_sl$times

# Predictions and super learner object
save_output(results, q_hat_sl, DIR, FILE_OUT_NAME)

# Super Learning results
cbind(Risk = q_hat_sl$cvRisk, Coef = q_hat_sl$coef) %>% write.csv(paste0(DIR, 'sl_summary.csv'), row.names = FALSE)

# CV Super Learning results
summary(cv_q_hat_sl)$Table %>% write.csv(paste0(DIR, 'sl_cv_summary.csv'), row.names = FALSE)

cbind(Risk = q_hat_sl$cvRisk, Coef = q_hat_sl$coef)
summary(cv_q_hat_sl)$Table$Ave 

risk <- data.frame(cbind(Risk = q_hat_sl$cvRisk, Coef = q_hat_sl$coef))
risk$Algorithm <- as.factor(row.names(risk))
cvrisk <- data.frame(summary(cv_q_hat_sl)$Table)
```


